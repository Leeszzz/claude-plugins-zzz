# Detailed 模式输出模板

此模板定义了详细模式（学术分析）的单篇文献报告格式。

---

## 文档结构

```markdown
# 文献详细分析：{title}

> 原始文件: {filename}
> 分析时间: {timestamp}
> 分析模式: 详细

---

## 📋 元信息

| 字段 | 内容 |
|------|------|
| 标题 | {title} |
| 作者 | {authors} |
| 年份 | {year} |
| 期刊/会议 | {venue} |
| DOI/链接 | {doi_or_link} |
| 关键词 | {keywords} |

---

## 1. 研究背景与动机

### 1.1 研究问题

{research_question}

### 1.2 现有工作的不足

{existing_limitations}

---

## 2. 研究方法

### 2.1 整体框架

{overall_framework}

### 2.2 关键技术

{key_techniques}

### 2.3 实验设置

{experimental_setup}

### 2.4 方法架构图

```mermaid
flowchart LR
    A[输入数据] --> B[预处理模块]
    B --> C[核心算法/模型]
    C --> D[后处理模块]
    D --> E[输出结果]

    style C fill:#e1f5ff,stroke:#01579b,stroke-width:2px
```

> 💡 **说明**: 此架构图展示论文提出方法的整体流程，核心算法部分用蓝色高亮标注。实际使用时请根据论文内容调整各模块名称。

---

## 3. 主要结果

### 3.1 定量结果

{quantitative_results}

### 3.2 定性分析

{qualitative_analysis}

---

## 4. 讨论与结论

### 4.1 核心贡献

{core_contributions}

### 4.2 实际意义

{practical_implications}

---

## 5. 局限性与未来工作

### 5.1 当前局限

{limitations}

### 5.2 未来方向

{future_work}

---

## 📝 个人笔记

<!-- 在此添加你的想法和笔记 -->

---

## 📚 相关文献

{references_if_available}

```

---

## 字段说明

| 字段 | 说明 | 必填 |
|------|------|------|
| `title` | 论文完整标题 | ✅ |
| `filename` | 原始文件名 | ✅ |
| `authors` | 完整作者列表 | ✅ |
| `year` | 发表年份 | ✅ |
| `venue` | 期刊或会议名称 | 可选 |
| `doi_or_link` | DOI 或论文链接 | 可选 |
| `keywords` | 关键词列表 | 可选 |
| 各章节内容 | 分析内容 | ✅ |

---

## 文件命名规则

详细模式为每篇文献生成单独文件：

```
输入文件: attention-is-all-you-need.pdf
输出文件: attention-is-all-you-need-summary.md

输入文件: bert_paper.md
输出文件: bert_paper-summary.md
```

---

## 内容要求

> ⚠️ **真实性第一原则**：所有内容必须严格来源于论文原文，禁止推测、解读或补充。如论文缺失某项内容，必须明确注明"论文中未提及"，绝不编造。

### 研究背景（约 200-300 字）
- 简述研究领域的背景
- 明确指出要解决的问题
- 总结现有方法的不足
- **如论文未提及背景或现有工作，注明"论文中未详细讨论研究背景"**

### 研究方法（约 300-500 字）
- 描述整体方法框架
- 解释关键技术创新点
- 说明实验设置（数据集、指标、基线）
- **方法架构图**：使用 Mermaid 流程图展示方法的整体流程（可根据实际论文内容调整节点和流程）
- **如实验设置不详，注明"论文中未详细说明实验设置"**

### 主要结果（约 200-400 字）
- 列出关键定量指标
- 提供定性分析或案例
- **如论文未提供定量结果，注明"论文中未提供定量对比数据"**

### 讨论与结论（约 200-300 字）
- 总结核心贡献（3-5 点）
- 讨论实际应用价值

### 局限性与未来（约 100-200 字）
- 指出论文承认的局限
- 列举可能的后续方向
- **如论文未讨论局限性或未来工作，必须注明：**
  - "论文中未讨论方法局限性"
  - "论文中未提及未来研究方向"

---

## 示例输出

```markdown
# 文献详细分析：Attention Is All You Need

> 原始文件: attention.pdf
> 分析时间: 2024-01-07 14:30:00
> 分析模式: 详细

---

## 📋 元信息

| 字段 | 内容 |
|------|------|
| 标题 | Attention Is All You Need |
| 作者 | Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. |
| 年份 | 2017 |
| 期刊/会议 | NeurIPS 2017 |
| DOI/链接 | arXiv:1706.03762 |
| 关键词 | Transformer, Attention, Sequence-to-Sequence |

---

## 1. 研究背景与动机

### 1.1 研究问题

序列到序列建模（如机器翻译）传统上依赖 RNN 或 CNN，这些方法在处理长距离依赖时存在困难，且难以并行化训练。本文探索能否完全依赖注意力机制来建模序列。

### 1.2 现有工作的不足

- RNN 的顺序性限制了并行训练
- CNN 需要多层才能捕捉远距离依赖
- 现有注意力机制仅作为 RNN 的补充，而非替代

---

## 2. 研究方法

### 2.1 整体框架

Transformer 采用编码器-解码器架构，每个编码器层包含多头自注意力和前馈网络，解码器额外增加编码器-解码器注意力层。

### 2.2 关键技术

1. **多头注意力**：将注意力分成多个头并行计算，捕捉不同子空间的特征
2. **位置编码**：使用正弦函数编码位置信息，弥补注意力机制缺乏位置感知
3. **残差连接与层归一化**：稳定深层网络训练

### 2.3 实验设置

- 数据集：WMT 2014 英德/英法翻译
- 指标：BLEU 分数
- 基线：ConvS2S、GNMT 等

### 2.4 方法架构图

```mermaid
flowchart LR
    A[源语言序列] --> B[编码器层×6]
    B --> C[多头自注意力]
    C --> D[解码器层×6]
    D --> E[目标语言序列]

    style C fill:#e1f5ff,stroke:#01579b,stroke-width:2px
```

> 💡 **说明**: Transformer 采用编码器-解码器架构，核心创新在于多头自注意力机制，完全替代了传统的 RNN 结构。

---

## 3. 主要结果

### 3.1 定量结果

| 任务 | 模型 | BLEU |
|------|------|------|
| 英德 | Transformer (big) | 28.4 |
| 英法 | Transformer (big) | 41.0 |

超越所有现有模型，且训练时间大幅减少。

### 3.2 定性分析

注意力可视化显示模型能够学习到语法结构和长距离依赖关系。

---

## 4. 讨论与结论

### 4.1 核心贡献

1. 提出完全基于注意力的 Transformer 架构
2. 实现高度并行化训练
3. 在翻译任务上达到 SOTA
4. 为后续 BERT、GPT 等模型奠定基础

### 4.2 实际意义

Transformer 已成为 NLP 领域的基础架构，广泛应用于机器翻译、文本生成、问答等任务。

---

## 5. 局限性与未来工作

### 5.1 当前局限

- 自注意力复杂度为 O(n²)，对超长序列不友好
- 缺乏归纳偏置，需要大量数据训练

### 5.2 未来方向

- 稀疏注意力机制
- 预训练语言模型
- 多模态 Transformer

---

## 📝 个人笔记

<!-- 在此添加你的想法和笔记 -->

```